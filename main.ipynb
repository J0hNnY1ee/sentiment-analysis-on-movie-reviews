{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1659,  3700, 11325, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [11407, 21002, 13289,   422,   607,  3350,   290, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), tensor([2, 4])]\n"
     ]
    }
   ],
   "source": [
    "import torch, tiktoken\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class reviewsDataset(Dataset):\n",
    "    def __init__(self, data_path, encoder, max_length=48, is_train=True):\n",
    "        self.phrase_ids = []\n",
    "        self.sentence_ids = []\n",
    "        self.phrases = []\n",
    "        self.sentiments = []\n",
    "        self.enc = encoder\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "        self.load_data(data_path)\n",
    "        self.tokenized_phrases = self.tokenize_phrases()\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "        # 将每一列保存到一个独立的变量中\n",
    "        self.phrase_ids = df[\"PhraseId\"].values\n",
    "        self.sentence_ids = df[\"SentenceId\"].values\n",
    "        self.phrases = df[\"Phrase\"].values\n",
    "        if self.is_train:\n",
    "            # 训练数据集包含情感标签\n",
    "            self.sentiments = df[\"Sentiment\"].values\n",
    "        else:\n",
    "            self.sentiments = [0] * len(df)\n",
    "\n",
    "    def tokenize_phrases(self):\n",
    "        # 对每个句子进行编码，并进行填充或截断以确保长度一致\n",
    "        tokenized_phrases = []\n",
    "        for phrase in self.phrases:\n",
    "            if not isinstance(phrase, str):\n",
    "                print(f\"Warning: Non-string value found: {phrase}\")\n",
    "                # 如果不是字符串，可以跳过或者进行默认处理\n",
    "                tokens = self.enc.encode(\"no string\")\n",
    "            else:\n",
    "                tokens = self.enc.encode(phrase)\n",
    "            # 截断\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[: self.max_length]\n",
    "            # 填充\n",
    "            elif len(tokens) < self.max_length:\n",
    "                tokens += [50256] * (\n",
    "                    self.max_length - len(tokens)\n",
    "                )  # 使用 gpt2 的填充 token\n",
    "            tokenized_phrases.append(tokens)\n",
    "        return tokenized_phrases\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phrase_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.tokenized_phrases[index]), torch.tensor(\n",
    "            self.sentiments[index]\n",
    "        )\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = reviewsDataset(\"train.tsv\", enc)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "print(next(iter(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,num_layers=2)\n",
    "        # 全连接层\n",
    "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        # input_sentences形状: (batch_size, seq_len)\n",
    "        \n",
    "        # 嵌入层\n",
    "        embedded = self.embedding(input_sentences)  # embedded形状: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out形状: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 取最后一个时间步的输出\n",
    "        final_hidden_state = lstm_out[:, -1, :]  # 形状: (batch_size, hidden_dim)\n",
    "        \n",
    "        # 全连接层\n",
    "        logits = self.linear(final_hidden_state)  # 形状: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 双向LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # 设置为双向LSTM\n",
    "        )\n",
    "        # 全连接层，注意这里输入维度是hidden_dim * 2，因为双向LSTM会输出两个方向的隐藏状态\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        # input_sentences形状: (batch_size, seq_len)\n",
    "        \n",
    "        # 嵌入层\n",
    "        embedded = self.embedding(input_sentences)  # embedded形状: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # 双向LSTM层\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out形状: (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # 取最后一个时间步的输出，对于双向LSTM，最后一个时间步的输出已经包含了正向和反向的信息\n",
    "        final_hidden_state = lstm_out[:, -1, :]  # 形状: (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        # 全连接层\n",
    "        logits = self.linear(final_hidden_state)  # 形状: (batch_size, num_classes)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    vocab_size, embedding_dim, hidden_dim, num_classes, data_loader, lr=0.001, epochs=5\n",
    "):\n",
    "    model = LSTM(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # outputs 是 logits\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 计算正确率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                accuracy = 100 * correct / total\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, iter {i}, loss: {loss.item():.4f}, accuracy: {accuracy:.2f}%\"\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, loss: 1.6196, accuracy: 26.56%\n",
      "Epoch 0, iter 100, loss: 1.2696, accuracy: 45.75%\n",
      "Epoch 0, iter 200, loss: 1.2619, accuracy: 48.13%\n",
      "Epoch 0, iter 300, loss: 1.3358, accuracy: 49.22%\n",
      "Epoch 0, iter 400, loss: 1.2954, accuracy: 49.70%\n",
      "Epoch 0, iter 500, loss: 1.3903, accuracy: 49.85%\n",
      "Epoch 0, iter 600, loss: 1.2897, accuracy: 49.98%\n",
      "Epoch 0, iter 700, loss: 1.2859, accuracy: 50.17%\n",
      "Epoch 0, iter 800, loss: 1.2557, accuracy: 50.29%\n",
      "Epoch 0, iter 900, loss: 1.2805, accuracy: 50.35%\n",
      "Epoch 0, iter 1000, loss: 1.1612, accuracy: 50.51%\n",
      "Epoch 0, iter 1100, loss: 1.0929, accuracy: 50.60%\n",
      "Epoch 0, iter 1200, loss: 1.1827, accuracy: 50.67%\n",
      "Epoch 1, iter 0, loss: 1.1969, accuracy: 51.56%\n",
      "Epoch 1, iter 100, loss: 1.1869, accuracy: 51.82%\n",
      "Epoch 1, iter 200, loss: 1.2884, accuracy: 51.65%\n",
      "Epoch 1, iter 300, loss: 1.2856, accuracy: 51.65%\n",
      "Epoch 1, iter 400, loss: 1.2850, accuracy: 51.65%\n",
      "Epoch 1, iter 500, loss: 1.2816, accuracy: 51.71%\n",
      "Epoch 1, iter 600, loss: 1.1959, accuracy: 51.68%\n",
      "Epoch 1, iter 700, loss: 1.2822, accuracy: 51.62%\n",
      "Epoch 1, iter 800, loss: 1.1086, accuracy: 51.69%\n",
      "Epoch 1, iter 900, loss: 1.1715, accuracy: 51.72%\n",
      "Epoch 1, iter 1000, loss: 1.2620, accuracy: 51.74%\n",
      "Epoch 1, iter 1100, loss: 1.1402, accuracy: 51.65%\n",
      "Epoch 1, iter 1200, loss: 1.2944, accuracy: 51.62%\n",
      "Epoch 2, iter 0, loss: 1.1802, accuracy: 50.78%\n",
      "Epoch 2, iter 100, loss: 1.2050, accuracy: 51.89%\n",
      "Epoch 2, iter 200, loss: 1.2321, accuracy: 51.56%\n",
      "Epoch 2, iter 300, loss: 1.1742, accuracy: 51.69%\n",
      "Epoch 2, iter 400, loss: 1.1403, accuracy: 51.84%\n",
      "Epoch 2, iter 500, loss: 1.1919, accuracy: 51.77%\n",
      "Epoch 2, iter 600, loss: 1.1017, accuracy: 51.90%\n",
      "Epoch 2, iter 700, loss: 1.1265, accuracy: 51.92%\n",
      "Epoch 2, iter 800, loss: 1.2430, accuracy: 51.92%\n",
      "Epoch 2, iter 900, loss: 1.1888, accuracy: 51.92%\n",
      "Epoch 2, iter 1000, loss: 1.0412, accuracy: 51.90%\n",
      "Epoch 2, iter 1100, loss: 1.1750, accuracy: 51.86%\n",
      "Epoch 2, iter 1200, loss: 1.0923, accuracy: 51.83%\n",
      "Epoch 3, iter 0, loss: 1.2373, accuracy: 44.53%\n",
      "Epoch 3, iter 100, loss: 1.2493, accuracy: 52.33%\n",
      "Epoch 3, iter 200, loss: 1.1002, accuracy: 52.32%\n",
      "Epoch 3, iter 300, loss: 1.3080, accuracy: 52.27%\n",
      "Epoch 3, iter 400, loss: 1.1410, accuracy: 51.95%\n",
      "Epoch 3, iter 500, loss: 1.2579, accuracy: 52.07%\n",
      "Epoch 3, iter 600, loss: 1.1395, accuracy: 51.92%\n",
      "Epoch 3, iter 700, loss: 1.1947, accuracy: 51.97%\n",
      "Epoch 3, iter 800, loss: 1.0418, accuracy: 52.00%\n",
      "Epoch 3, iter 900, loss: 1.2021, accuracy: 52.02%\n",
      "Epoch 3, iter 1000, loss: 1.2002, accuracy: 52.11%\n",
      "Epoch 3, iter 1100, loss: 1.1872, accuracy: 52.13%\n",
      "Epoch 3, iter 1200, loss: 1.2675, accuracy: 52.07%\n",
      "Epoch 4, iter 0, loss: 1.0681, accuracy: 59.38%\n",
      "Epoch 4, iter 100, loss: 1.1719, accuracy: 52.63%\n",
      "Epoch 4, iter 200, loss: 1.2321, accuracy: 52.37%\n",
      "Epoch 4, iter 300, loss: 1.2526, accuracy: 52.28%\n",
      "Epoch 4, iter 400, loss: 1.2394, accuracy: 52.23%\n",
      "Epoch 4, iter 500, loss: 1.0525, accuracy: 52.32%\n",
      "Epoch 4, iter 600, loss: 1.1124, accuracy: 52.28%\n",
      "Epoch 4, iter 700, loss: 1.1984, accuracy: 52.31%\n",
      "Epoch 4, iter 800, loss: 1.2112, accuracy: 52.45%\n",
      "Epoch 4, iter 900, loss: 1.1720, accuracy: 52.43%\n",
      "Epoch 4, iter 1000, loss: 1.2425, accuracy: 52.53%\n",
      "Epoch 4, iter 1100, loss: 1.1436, accuracy: 52.60%\n",
      "Epoch 4, iter 1200, loss: 1.2243, accuracy: 52.56%\n",
      "Epoch 5, iter 0, loss: 1.2155, accuracy: 48.44%\n",
      "Epoch 5, iter 100, loss: 1.2104, accuracy: 52.82%\n",
      "Epoch 5, iter 200, loss: 1.1011, accuracy: 52.83%\n",
      "Epoch 5, iter 300, loss: 1.3363, accuracy: 52.80%\n",
      "Epoch 5, iter 400, loss: 1.3098, accuracy: 52.96%\n",
      "Epoch 5, iter 500, loss: 1.0751, accuracy: 52.96%\n",
      "Epoch 5, iter 600, loss: 1.2758, accuracy: 52.86%\n",
      "Epoch 5, iter 700, loss: 1.2151, accuracy: 52.85%\n",
      "Epoch 5, iter 800, loss: 1.1970, accuracy: 52.88%\n",
      "Epoch 5, iter 900, loss: 1.0937, accuracy: 52.99%\n",
      "Epoch 5, iter 1000, loss: 1.2009, accuracy: 53.01%\n",
      "Epoch 5, iter 1100, loss: 1.0918, accuracy: 53.05%\n",
      "Epoch 5, iter 1200, loss: 1.1252, accuracy: 53.08%\n",
      "Epoch 6, iter 0, loss: 1.2043, accuracy: 50.78%\n",
      "Epoch 6, iter 100, loss: 1.1082, accuracy: 53.13%\n",
      "Epoch 6, iter 200, loss: 1.1547, accuracy: 53.52%\n",
      "Epoch 6, iter 300, loss: 1.1061, accuracy: 53.35%\n",
      "Epoch 6, iter 400, loss: 1.0101, accuracy: 53.34%\n",
      "Epoch 6, iter 500, loss: 1.2222, accuracy: 53.39%\n",
      "Epoch 6, iter 600, loss: 1.0482, accuracy: 53.33%\n",
      "Epoch 6, iter 700, loss: 1.0842, accuracy: 53.39%\n",
      "Epoch 6, iter 800, loss: 1.0441, accuracy: 53.36%\n",
      "Epoch 6, iter 900, loss: 1.0497, accuracy: 53.49%\n",
      "Epoch 6, iter 1000, loss: 1.1911, accuracy: 53.54%\n",
      "Epoch 6, iter 1100, loss: 1.1325, accuracy: 53.54%\n",
      "Epoch 6, iter 1200, loss: 1.0786, accuracy: 53.66%\n",
      "Epoch 7, iter 0, loss: 1.1495, accuracy: 50.78%\n",
      "Epoch 7, iter 100, loss: 1.0857, accuracy: 53.64%\n",
      "Epoch 7, iter 200, loss: 1.0935, accuracy: 54.41%\n",
      "Epoch 7, iter 300, loss: 1.0785, accuracy: 54.21%\n",
      "Epoch 7, iter 400, loss: 1.0963, accuracy: 54.22%\n",
      "Epoch 7, iter 500, loss: 1.0322, accuracy: 54.31%\n",
      "Epoch 7, iter 600, loss: 1.1730, accuracy: 54.29%\n",
      "Epoch 7, iter 700, loss: 1.0345, accuracy: 54.22%\n",
      "Epoch 7, iter 800, loss: 1.0404, accuracy: 54.26%\n",
      "Epoch 7, iter 900, loss: 1.1873, accuracy: 54.26%\n",
      "Epoch 7, iter 1000, loss: 1.1276, accuracy: 54.36%\n",
      "Epoch 7, iter 1100, loss: 1.2599, accuracy: 54.37%\n",
      "Epoch 7, iter 1200, loss: 1.0761, accuracy: 54.41%\n",
      "Epoch 8, iter 0, loss: 1.1674, accuracy: 50.78%\n",
      "Epoch 8, iter 100, loss: 1.1021, accuracy: 54.79%\n",
      "Epoch 8, iter 200, loss: 1.0771, accuracy: 54.85%\n",
      "Epoch 8, iter 300, loss: 1.0807, accuracy: 54.92%\n",
      "Epoch 8, iter 400, loss: 1.1732, accuracy: 55.00%\n",
      "Epoch 8, iter 500, loss: 1.1597, accuracy: 55.01%\n",
      "Epoch 8, iter 600, loss: 1.1454, accuracy: 54.99%\n",
      "Epoch 8, iter 700, loss: 1.1217, accuracy: 55.03%\n",
      "Epoch 8, iter 800, loss: 1.0470, accuracy: 54.98%\n",
      "Epoch 8, iter 900, loss: 1.1365, accuracy: 55.08%\n",
      "Epoch 8, iter 1000, loss: 1.1479, accuracy: 55.13%\n",
      "Epoch 8, iter 1100, loss: 1.0986, accuracy: 55.10%\n",
      "Epoch 8, iter 1200, loss: 1.0258, accuracy: 55.10%\n",
      "Epoch 9, iter 0, loss: 1.1058, accuracy: 51.56%\n",
      "Epoch 9, iter 100, loss: 1.1087, accuracy: 56.36%\n",
      "Epoch 9, iter 200, loss: 1.0317, accuracy: 55.80%\n",
      "Epoch 9, iter 300, loss: 1.1019, accuracy: 55.62%\n",
      "Epoch 9, iter 400, loss: 1.0569, accuracy: 55.79%\n",
      "Epoch 9, iter 500, loss: 0.9525, accuracy: 55.83%\n",
      "Epoch 9, iter 600, loss: 1.1019, accuracy: 55.76%\n",
      "Epoch 9, iter 700, loss: 1.2177, accuracy: 55.82%\n",
      "Epoch 9, iter 800, loss: 1.0129, accuracy: 55.88%\n",
      "Epoch 9, iter 900, loss: 1.1255, accuracy: 55.87%\n",
      "Epoch 9, iter 1000, loss: 1.1419, accuracy: 55.91%\n",
      "Epoch 9, iter 1100, loss: 1.1543, accuracy: 55.88%\n",
      "Epoch 9, iter 1200, loss: 1.0300, accuracy: 55.87%\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "import tiktoken\n",
    "\n",
    "vocab_size = 50257  # GPT-2 的词汇表大小\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "num_classes = 5  # 假设有5个情感类别\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = reviewsDataset(\"train.tsv\", enc)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model = train(\n",
    "    vocab_size, embedding_dim, hidden_dim, num_classes, data_loader, lr=0.0001, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Non-string value found: nan\n",
      "预测结果已保存到 test_with_sentiment.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "def predict(model, test_path):\n",
    "    # 切换模型到评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 获取编码器\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    test_dataset = reviewsDataset(test_path, enc, is_train=False)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # 用于存储预测结果\n",
    "    predictions = []\n",
    "    \n",
    "    # 禁用梯度计算以提高效率\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            # 获取预测结果\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # 将预测结果添加到列表中\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 使用示例\n",
    "test_path = \"test.tsv\"\n",
    "output_path = \"test_with_sentiment.csv\"\n",
    "\n",
    "# 在 predict 函数外打开并读取原始测试数据\n",
    "test_df = pd.read_csv(test_path, sep='\\t')  # 假设测试数据是 TSV 格式\n",
    "\n",
    "# 确保只保留 'PhraseId' 和 'Sentiment' 两列\n",
    "# 如果原始数据中没有 'Sentiment' 列，可以直接创建一个空列\n",
    "if 'Sentiment' not in test_df.columns:\n",
    "    test_df['Sentiment'] = None\n",
    "\n",
    "# 获取预测结果\n",
    "predictions = predict(model, test_path)\n",
    "\n",
    "# 将预测结果添加到 'Sentiment' 列中\n",
    "test_df['Sentiment'] = predictions\n",
    "\n",
    "# 只保留 'PhraseId' 和 'Sentiment' 两列\n",
    "result_df = test_df[['PhraseId', 'Sentiment']]\n",
    "\n",
    "# 将结果保存到新的文件中\n",
    "result_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
